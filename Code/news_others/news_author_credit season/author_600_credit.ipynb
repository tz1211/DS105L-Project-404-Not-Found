{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_600_author = pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/news_author/cleaned_authors_updated/news_600_author_cl.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_600_author_1 = news_600_author.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def add_credit(dataframe_author):\n",
    "    author_list = list(dataframe_author[\"author\"])\n",
    "    credit_article = [\"None\"] * len(author_list)\n",
    "    credit_followers = [\"None\"] * len(author_list)\n",
    "    credit_tweets = [\"None\"] * len(author_list)\n",
    "    for i, names in enumerate(tqdm(author_list)):\n",
    "        if names == \"NaN\":\n",
    "            continue\n",
    "        elif \"NBA.com\" in str(names) or \"NBA News\" in str(names) or \"Press\" in str(names) or \"Official Release\" in str(names) or \"NBA Twitter\" in str(names):\n",
    "            credit_article[i] = \"Default\"\n",
    "            credit_followers[i] = \"Default\"\n",
    "            credit_tweets[i] = \"Default\"\n",
    "        else:\n",
    "            try:\n",
    "                player_search = str(names).replace(\" \", \"-\")\n",
    "                url = \"https://muckrack.com/\" + player_search\n",
    "                response = requests.get(url)\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                html = soup.find_all('a', attrs={'class':\"profile-edit-btn\"})\n",
    "                if len(html) == 0:\n",
    "                    num_articles = \"None\"\n",
    "                    credit_article[i] = num_articles\n",
    "                else:\n",
    "                    for line in html:\n",
    "                        if \"See all\" in line.text:\n",
    "                            span_elem = line.find('span')\n",
    "                            text_content = span_elem.text.strip()\n",
    "                            num_articles = text_content.split()[0].replace(',', '')\n",
    "                            credit_article[i] = num_articles\n",
    "                            break\n",
    "                    if credit_followers[i] == \"None\":\n",
    "                        credit_followers[i] = \"None\"\n",
    "                        \n",
    "                    tweet = soup.find('p', attrs={'class':\"fs-6 mt-0 mb-6\"})\n",
    "                    followers = tweet.find('strong').text.replace(',', '')\n",
    "                    tweets = tweet.find_all('strong')[1].text.replace(',', '')\n",
    "                    if len(followers) != 0:\n",
    "                        credit_followers[i] = followers\n",
    "                    if len(followers) == 0:\n",
    "                        followers = \"0\" \n",
    "                        credit_followers[i] = followers\n",
    "                    if len(tweets) != 0:\n",
    "                        credit_tweets[i] = tweets\n",
    "                    if len(tweets) == 0:\n",
    "                        tweets = \"0\"\n",
    "                        credit_tweets[i] = tweets\n",
    "            except Exception as e:\n",
    "                if credit_followers[i] == \"None\":\n",
    "                    credit_followers[i] = \"None\"\n",
    "                if credit_tweets[i] == \"None\":\n",
    "                    credit_tweets[i] = \"None\"\n",
    "    dataframe_author[\"credit_article\"] = credit_article\n",
    "    dataframe_author[\"credit_followers\"] = credit_followers\n",
    "    dataframe_author[\"credit_tweets\"] = credit_tweets\n",
    "    return dataframe_author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [05:54<00:00, 16.94it/s]  \n"
     ]
    }
   ],
   "source": [
    "news_600_author_cedit_1 = add_credit(news_600_author_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_600_author_cedit_1.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/author_credit_nba/news_600_author_credit_1.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
