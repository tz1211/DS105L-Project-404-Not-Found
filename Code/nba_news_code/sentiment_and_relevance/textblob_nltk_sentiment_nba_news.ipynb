{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/annabelitong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/annabelitong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/annabelitong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/annabelitong/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_nba_textblob_sentiment(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    content = soup.find(\"div\", attrs={\"class\": \"entry__content entry-content\"})\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    main_body = content.find_all('p')\n",
    "    text = \" \".join([p.get_text(strip=True) for p in main_body])\n",
    "    clean_text = text.replace('\\n', ' ').replace('\\xa0', '')\n",
    "    blob = TextBlob(clean_text)\n",
    "    sentiment = blob.sentiment\n",
    "    polarity = sentiment.polarity  # Assess sentiment polarity (-1 to 1)\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df600= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_600_credit.csv')\n",
    "df1200= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1200_credit.csv')\n",
    "df1600= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1600_credit.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df600['textblob_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df600.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df600.at[index, 'textblob_Sentiment'] = sentiment\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df600.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_600_credit_season_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1200['textblob_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df1200.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df1200.at[index, 'textblob_Sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1200.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1200_credit_season_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1600['textblob_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df1600.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df1600.at[index, 'textblob_Sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1600.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1600_credit_season_2.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK sentimets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_600= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_600_credit_season_2.csv')\n",
    "df_1200= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1200_credit_season_2.csv')\n",
    "df_1600= pd.read_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1600_credit_season_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_sentiment_nltk(text):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sentiment_analyzer.polarity_scores(text)\n",
    "    score = sentiment_scores['compound']\n",
    "    return score\n",
    "\n",
    "def get_nba_nltk_sentiment(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    content = soup.find(\"div\", attrs={\"class\": \"entry__content entry-content\"})\n",
    "\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    main_body = content.find_all('p')\n",
    "    text = \" \".join([p.get_text(strip=True) for p in main_body])\n",
    "    clean_text = text.replace('\\n', ' ').replace('\\xa0', '')\n",
    "    score = assess_sentiment_nltk(clean_text)\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_600['nltk_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_600.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df_600.at[index, 'nltk_Sentiment'] = sentiment\n",
    "\n",
    "df_600.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_600_credit_season_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1200['nltk_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_1200.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df_1200.at[index, 'nltk_Sentiment'] = sentiment\n",
    "\n",
    "df_1200.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1200_credit_season_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1600['nltk_Sentiment'] = None\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_1600.iterrows():\n",
    "    url = row['url']\n",
    "    sentiment = get_nba_textblob_sentiment(url)\n",
    "    df_1600.at[index, 'nltk_Sentiment'] = sentiment\n",
    "\n",
    "df_1600.to_csv('/Users/annabelitong/desktop/annabel/uni/ds105l/group_project/DS105L-Project/Data/nba_author_credit/nba_1600_credit_season_3.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "705d6f4ecb8a535ad130d19f33aca7c81049a16c0faf5a3ee87de4e6b987140d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
